{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70199977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python3.10 -m pip install --upgrade pip\n",
    "!python3.10 -m pip install torch torchvision torchaudio\n",
    "!python3.10 -m pip install transformers pymupdf pytesseract pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21db850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (25.1.1)\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.17.2-cp310-cp310-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.2.2-cp310-cp310-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-macosx_10_10_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Using cached torchvision-0.17.2-cp310-cp310-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "Using cached torchaudio-2.2.2-cp310-cp310-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl (14 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [torchaudio]2\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-11.2.1 sympy-1.14.0 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pymupdf\n",
      "  Using cached pymupdf-1.26.0-cp39-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (11.2.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-macosx_10_12_x86_64.whl.metadata (879 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached hf_xet-1.1.3-cp37-abi3-macosx_10_12_x86_64.whl (2.3 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "Using cached pymupdf-1.26.0-cp39-abi3-macosx_10_9_x86_64.whl (23.2 MB)\n",
      "Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl (184 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl (287 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl (201 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, pytesseract, pymupdf, idna, hf-xet, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [transformers][0m [transformers]ub]er]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset_normalizer-3.4.2 hf-xet-1.1.3 huggingface-hub-0.32.4 idna-3.10 pymupdf-1.26.0 pytesseract-0.3.13 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4 urllib3-2.4.0\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl (14 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp310-cp310-macosx_15_0_x86_64.whl size=3903783 sha256=74af3d2b7048e83944ec6a15fce51eb266713a5e88e0d28d86de9aada1bb49ec\n",
      "  Stored in directory: /private/var/folders/vm/7t2c20lj2yn0g5bbmhzw8c300000gn/T/pip-ephem-wheel-cache-c4c3t7lv/wheels/e2/91/0a/79c7b44fab10c7222ec91bd97fd7f6708beba84d5934228a80\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.032m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: jinja2m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.6━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.6━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [MarkupSafe]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [llama-cpp-python][llama-cpp-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.9 numpy-2.2.6 typing-extensions-4.14.0\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.10/site-packages (0.5.3)\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.6)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_x86_64.whl.metadata (20 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_x86_64.whl (56.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n",
    "!{sys.executable} -m pip install transformers pymupdf pytesseract pillow\n",
    "\n",
    "# LLM and text processing\n",
    "!{sys.executable} -m pip install --force-reinstall --no-cache-dir llama-cpp-python\n",
    "!{sys.executable} -m pip install safetensors\n",
    "\n",
    "# Additional utilities\n",
    "!{sys.executable} -m pip install tqdm numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c219ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: /Users/mac/Desktop/pdf_extracter/.venv/bin/python\n",
      "/Users/mac/.zshenv:1: command not found: 0x0:0x0\n",
      "llama_cpp_python   0.3.9\n",
      "safetensors        0.5.3\n",
      "torch              2.2.2\n",
      "torchaudio         2.2.2\n",
      "torchvision        0.17.2\n",
      "transformers       4.52.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "!{sys.executable} -m pip list | grep -E \"torch|transformers|pymupdf|llama|safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730934c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 560X) - 4087 MiB free\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /Users/mac/Desktop/pdf_extracter/tinyllama-1.1b-chat-v1.0.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q6_K:  156 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q6_K\n",
      "print_info: file size   = 860.86 MiB (6.56 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 22\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 5632\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.10 B\n",
      "print_info: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 200 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/23 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   860.86 MiB\n",
      "..........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: AMD Radeon Pro 560X\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 560X\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 560X\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: simdgroup reduction   = false\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = false\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4294.97 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7fcedc44ff20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row                             0x7fcedc426940 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7fcedc598740 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub_row                             0x7fcedc5f4a20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7fcee9a271d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul_row                             0x7fcedcb673f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div                                 0x7fcedc790080 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div_row                             0x7fcedca068c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7fcedcbe7160 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7fcedc7acd30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7fcedc7e60a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7fcedc4d4ea0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale                               0x7fcedcbe7340 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7fcedcbe7520 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7fcedcbe7700 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7fcedc4a0d50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_relu                                0x7fcedc637480 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7fcedc442000 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7fcee84eb7c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7fcedcbe78e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7fcedc4471a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7fcedc5a3a90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu                                0x7fcedc422020 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7fcedc411890 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7fcedc637660 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_soft_max_f16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f16_4                    (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32_4                    (not supported)\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7fcedc591e30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7fcedc4cc2e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7fcedc7be680 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7fcedc637840 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7fcedc6372a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7fcedc5940a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7fcedc7db050 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7fcedc4ecf40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7fcedc44b8b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7fcedc637c40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7fcedca06e20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7fcedc5f2f10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7fcedc5681f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7fcedc45eb70 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7fcee9a4c6d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7fcedca25b20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7fcedc55a030 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7fcedcbe7ac0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7fcedc4b5250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7fcedc5ad690 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7fcedc5f4cc0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7fcedc5d9f70 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7fcedc59b1d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7fcedc58eca0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_rms_norm                          (not supported)\n",
      "ggml_metal_init: skipping kernel_l2_norm                           (not supported)\n",
      "ggml_metal_init: skipping kernel_group_norm                        (not supported)\n",
      "ggml_metal_init: loaded kernel_norm                                0x7fcedcbe7ca0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7fcedc637e20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7fcedc638170 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                       0x7fcee9a9c240 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                       0x7fcedc47f150 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_mul_mv_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_1row               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_l4                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f16                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_2           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_3           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_4           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_5           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_2        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_3        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_4        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_5        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7fcedc8d8380 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7fcee99da450 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7fcee99d8ea0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7fcedc705a00 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7fcedc4cfb80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7fcedc5535f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7fcedc4e6a30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7fcedc638350 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7fcedcbe8000 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7fcedc638530 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7fcedc638710 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7fcedc45c650 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7fcedc4ed460 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7fcedcbe81e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7fcedcbe83c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7fcedc5c3010 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7fcedc6388f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7fcedc4d3ba0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h192           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk192_hv128    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk576_hv512    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h96        (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h128       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h192       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h256       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7fcf1e123360 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7fcedca54af0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7fcedcbe85a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7fcedc7d71d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7fcedc7ed080 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7fcedc638ad0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7fcedc7f6360 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7fcedc7a4ef0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7fcedc638cb0 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7fcedc638e90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7fcedc639070 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7fcedcbe8780 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                        0x7fcee9a90ce0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                        0x7fcee9ad48c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                        0x7fcedc639250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                        0x7fcedc7231f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                        0x7fcedc7ce540 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                        0x7fcedc734e60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                        0x7fcedc4aa340 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                        0x7fcedc5bc250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                        0x7fcedc5c0370 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                        0x7fcedc414580 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_concat                              0x7fcedc639430 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7fcedc639610 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7fcedc79e440 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7fcedc6397f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7fcedc76dc20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_neg                                 0x7fcedc6399d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7fcedc41f1a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7fcedc639bb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7fcedc4f2d60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7fcee9ad4aa0 | th_max = 1024 | th_width =   64\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 22, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    11.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    66.50 MiB\n",
      "llama_context: graph nodes  = 754\n",
      "llama_context: graph splits = 354 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '4', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '64', 'general.file_type': '18', 'llama.feed_forward_length': '5632', 'llama.embedding_length': '2048', 'llama.block_count': '22', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"/Users/mac/Desktop/pdf_extracter/tinyllama-1.1b-chat-v1.0.Q6_K.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c3d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz\n",
    "import torch\n",
    "from PIL import Image\n",
    "from llama_cpp import Llama\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from typing import Dict, List, Optional, Tuple, Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2b03",
   "metadata": {},
   "source": [
    "### Initialising BLIP and TinyLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca29f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(model_path: str) -> Tuple[Llama, BlipProcessor, BlipForConditionalGeneration]:\n",
    "    \"\"\"Initialize LLM and BLIP models\"\"\"\n",
    "    llm = Llama(model_path=model_path)\n",
    "    processor = BlipProcessor.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\",\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\",\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    return llm, processor, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70d006",
   "metadata": {},
   "source": [
    "### Chunking page text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f71864ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_length: int = 400) -> List[str]:\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip() + '.'\n",
    "        if current_length + len(sentence) > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(sentence)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb27e1",
   "metadata": {},
   "source": [
    "### Summarising text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca26dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, llm: Llama) -> str:\n",
    "    \"\"\"Generate a well-structured summary using LLM\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Empty text\"\n",
    "        \n",
    "    chunks = chunk_text(text, max_length=300)  # Reduced for better context\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        prompt = \"\"\"Please provide a comprehensive summary of the following text. \n",
    "Focus on key points and maintain clarity:\n",
    "\n",
    "Text to summarize:\n",
    "----------------\n",
    "{text}\n",
    "----------------\n",
    "\n",
    "Guidelines:\n",
    "- Capture main ideas and important details\n",
    "- Use clear, professional language\n",
    "- Maintain logical flow\n",
    "- Be concise but informative\n",
    "\n",
    "Summary:\"\"\".format(text=chunk)\n",
    "\n",
    "        try:\n",
    "            response = llm(\n",
    "                prompt, \n",
    "                max_tokens=150,\n",
    "                stop=[\"----------------\", \"\\n\\n\"],\n",
    "                temperature=0.3,  # Reduced for more focused output\n",
    "                top_p=0.9,\n",
    "                repeat_penalty=1.2\n",
    "            )\n",
    "            summary = response[\"choices\"][0][\"text\"].strip()\n",
    "            if summary:\n",
    "                summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing chunk: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not summaries:\n",
    "        return \"Summary generation failed\"\n",
    "    \n",
    "    # Combine summaries with proper formatting\n",
    "    final_summary = \" \".join(summaries)\n",
    "    \n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4a9dc",
   "metadata": {},
   "source": [
    "### Preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c2903ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, target_size: Tuple[int, int] = (384, 384)) -> Optional[Image.Image]:\n",
    "    \"\"\"Preprocess and resize image\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            img.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
    "            new_img = Image.new('RGB', target_size, (255, 255, 255))\n",
    "            offset = ((target_size[0] - img.size[0]) // 2,\n",
    "                     (target_size[1] - img.size[1]) // 2)\n",
    "            new_img.paste(img, offset)\n",
    "            return new_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbc70f",
   "metadata": {},
   "source": [
    "### Generating image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4df29275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image: Image.Image, processor: BlipProcessor, \n",
    "                         model: BlipForConditionalGeneration) -> str:\n",
    "    \"\"\"Generate caption for an image using BLIP\"\"\"\n",
    "    try:\n",
    "        inputs = processor(\n",
    "            image, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "        \n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=30,\n",
    "            num_beams=4,\n",
    "            min_length=5,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        return processor.decode(out[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption: {e}\")\n",
    "        return \"Caption generation failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29652b08",
   "metadata": {},
   "source": [
    "### Saving data to output_dir and Metadata to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "336c181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(output_dir: str, metadata: Dict[str, Any], \n",
    "                full_text: str, summaries: Dict[int, str]) -> None:\n",
    "    \"\"\"Save all outputs to files\"\"\"\n",
    "    # Save text\n",
    "    text_file = os.path.join(output_dir, \"extracted_text.txt\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "    # Save summaries\n",
    "    summaries_file = os.path.join(output_dir, \"page_summaries.txt\")\n",
    "    with open(summaries_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"PDF CONTENT SUMMARIES\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        for page_num, summary in summaries.items():\n",
    "            f.write(f\"Page {page_num}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"{summary}\\n\\n\")\n",
    "\n",
    "    # Save metadata\n",
    "    json_file = os.path.join(output_dir, \"summary.json\")\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30fa55",
   "metadata": {},
   "source": [
    "### storing image binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60a129ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page_image(img: Dict, page_num: int, img_index: int, \n",
    "                      doc: fitz.Document, output_dir: str) -> Optional[str]:\n",
    "    \"\"\"Process a single image from a PDF page\"\"\"\n",
    "    try:\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_filename = f\"page_{page_num}_img_{img_index+1}.{base_image['ext']}\"\n",
    "        image_path = os.path.join(output_dir, image_filename)\n",
    "        \n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(base_image[\"image\"])\n",
    "        print(f\"Saved: {image_path}\")\n",
    "        return image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339281e",
   "metadata": {},
   "source": [
    "### Running All functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46002241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path: str, output_dir: str = \"output1\", \n",
    "                       model_path: str = \"/Users/mac/Desktop/pdf_extracter/tinyllama-1.1b-chat-v1.0.Q6_K.gguf\") -> Dict[str, Any]:\n",
    "    \"\"\"Main function to extract and process PDF content\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    llm, processor, model = initialize_models(model_path)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    total_images = 0\n",
    "    image_captions = []\n",
    "    summaries = {}\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        # Process text\n",
    "        page_text = page.get_text()\n",
    "        full_text += f\"\\n--- Page {page_num} ---\\n{page_text}\"\n",
    "        print(f\"Summarizing page {page_num}...\")\n",
    "        summaries[page_num] = summarize_text(page_text, llm)\n",
    "\n",
    "        # Process images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            image_path = process_page_image(img, page_num, img_index, doc, output_dir)\n",
    "            if image_path:\n",
    "                total_images += 1\n",
    "                raw_image = preprocess_image(image_path)\n",
    "                if raw_image:\n",
    "                    caption = generate_image_caption(raw_image, processor, model)\n",
    "                    image_captions.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"image\": os.path.basename(image_path),\n",
    "                        \"caption\": caption\n",
    "                    })\n",
    "\n",
    "    metadata = {\n",
    "        \"total_pages\": len(doc),\n",
    "        \"total_images\": total_images,\n",
    "        \"text_file\": \"extracted_text.txt\",\n",
    "        \"image_captions\": image_captions,\n",
    "        \"summaries\": summaries\n",
    "    }\n",
    "\n",
    "    save_outputs(output_dir, metadata, full_text, summaries)\n",
    "    return metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e1c83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 560X) - 4087 MiB free\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /Users/mac/Desktop/pdf_extracter/tinyllama-1.1b-chat-v1.0.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q6_K:  156 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q6_K\n",
      "print_info: file size   = 860.86 MiB (6.56 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 22\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 5632\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.10 B\n",
      "print_info: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 200 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/23 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   860.86 MiB\n",
      "..........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: AMD Radeon Pro 560X\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 560X\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 560X\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: simdgroup reduction   = false\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = false\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4294.97 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7fcee9acf770 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row                             0x7fcee9acf950 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7fcee9acfb30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub_row                             0x7fcee9acfd10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7fcee8696e20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul_row                             0x7fcee868a3c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div                                 0x7fcee8692520 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div_row                             0x7fcedc4b1a30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7fcee8693300 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7fcf4e836bb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7fcedc432ad0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7fcee868d370 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale                               0x7fcee8694460 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7fcee9acfef0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7fcee9ad00d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7fcedc4e0af0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_relu                                0x7fcedc411de0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7fcedc4081f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7fcedc4b1210 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7fcee8764780 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7fcedc4b1d20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7fcedcbfab50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu                                0x7fcedcbfad60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7fcedcbfa970 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7fcee8764960 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_soft_max_f16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f16_4                    (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32_4                    (not supported)\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7fcedc53eef0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7fcf4e83ae90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7fcedc5e1720 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7fcedc4a2120 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7fcee9ad02e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7fcee8696080 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7fcee83c74a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7fcee9ad04c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7fcedcbfb130 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7fcedc43b420 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7fcee83c3ee0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7fcedc46cbd0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7fcee9ad06a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7fcee83c8400 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7fcee83c3710 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7fcee83c8950 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7fcee83eb950 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7fcee868ee90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7fcee8764b40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7fcee8764d20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7fcee868c590 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7fcedc496150 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7fcee868dfd0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7fcee86914c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_rms_norm                          (not supported)\n",
      "ggml_metal_init: skipping kernel_l2_norm                           (not supported)\n",
      "ggml_metal_init: skipping kernel_group_norm                        (not supported)\n",
      "ggml_metal_init: loaded kernel_norm                                0x7fcedc687a30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7fcee9ad0880 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7fcee9ad0a60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                       0x7fcedc687c10 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                       0x7fcee83be720 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_mul_mv_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_1row               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_l4                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f16                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_2           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_3           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_4           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_5           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_2        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_3        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_4        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_5        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7fcee8764f00 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7fcee87650e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7fcedc687df0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7fcedc687fd0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7fcedc4dd6f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7fcedcbfa6d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7fcee83c30e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7fcedcbfb500 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7fcf4e83a9f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7fcee8765440 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7fcedc6881b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7fcedcbfb850 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7fcee8765620 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7fcee8765800 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7fcedcbfaf40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7fcee87659e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7fcee8765bc0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7fcee8765da0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h192           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk192_hv128    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk576_hv512    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h96        (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h128       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h192       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h256       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7fcee9ad0c40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7fcedc688390 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7fcedc47fd40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7fcedc688570 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7fcedcbfbba0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7fcedcbfbd80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7fcedcbfbf60 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7fcee83c66c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7fcedc688750 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7fcee8765f80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7fcedc4d03d0 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7fcedc4d0be0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                        0x7fcee83c5900 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                        0x7fcedc4a3710 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                        0x7fcf4ea47560 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                        0x7fcee9ad0e20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                        0x7fcee9ad1000 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                        0x7fcee9ad11e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                        0x7fcee9ad13c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                        0x7fcee9ad15a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                        0x7fcee9ad1780 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                        0x7fcedc497440 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_concat                              0x7fcedc445870 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7fcedc4e1810 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7fcee9ad1960 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7fcee8766160 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7fcedc4c7520 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_neg                                 0x7fcee9ad1b40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7fcee83c4c20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7fcee9ad1d20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7fcedc688930 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7fcee8766340 | th_max = 1024 | th_width =   64\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 22, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    11.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    66.50 MiB\n",
      "llama_context: graph nodes  = 754\n",
      "llama_context: graph splits = 354 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '4', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '64', 'general.file_type': '18', 'llama.feed_forward_length': '5632', 'llama.embedding_length': '2048', 'llama.block_count': '22', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2872.71 ms /   100 tokens (   28.73 ms per token,    34.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.11 ms /   101 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2261.98 ms /    99 tokens (   22.85 ms per token,    43.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     280.89 ms /     7 runs   (   40.13 ms per token,    24.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    2544.92 ms /   106 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2212.74 ms /    57 tokens (   38.82 ms per token,    25.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2213.92 ms /    58 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    2753.59 ms /   150 tokens (   18.36 ms per token,    54.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4387.36 ms /   104 runs   (   42.19 ms per token,    23.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    7173.55 ms /   254 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1543.98 ms /   121 tokens (   12.76 ms per token,    78.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1545.23 ms /   122 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output1/page_1_img_1.jpeg\n",
      "Summarizing page 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 16 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1533.02 ms /   121 tokens (   12.67 ms per token,    78.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1533.79 ms /   122 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1917.07 ms /    95 tokens (   20.18 ms per token,    49.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2798.81 ms /    71 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    4735.95 ms /   166 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1515.58 ms /   102 tokens (   14.86 ms per token,    67.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2724.27 ms /    71 runs   (   38.37 ms per token,    26.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    4260.68 ms /   173 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1783.18 ms /   109 tokens (   16.36 ms per token,    61.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =      40.03 ms /     1 runs   (   40.03 ms per token,    24.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1824.48 ms /   110 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1045.51 ms /    44 tokens (   23.76 ms per token,    42.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1082.33 ms /    26 runs   (   41.63 ms per token,    24.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    2135.07 ms /    70 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1637.59 ms /    97 tokens (   16.88 ms per token,    59.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4978.20 ms /   120 runs   (   41.48 ms per token,    24.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    6659.71 ms /   217 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1959.31 ms /   118 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1960.56 ms /   119 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1762.27 ms /    79 tokens (   22.31 ms per token,    44.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1763.43 ms /    80 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1454.07 ms /    66 tokens (   22.03 ms per token,    45.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1455.26 ms /    67 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output1/page_2_img_1.jpeg\n",
      "Summarizing page 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 16 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1713.30 ms /    99 tokens (   17.31 ms per token,    57.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1714.28 ms /   100 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1628.47 ms /   111 tokens (   14.67 ms per token,    68.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2957.53 ms /    70 runs   (   42.25 ms per token,    23.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    4607.31 ms /   181 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1386.37 ms /    85 tokens (   16.31 ms per token,    61.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3002.63 ms /    66 runs   (   45.49 ms per token,    21.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    4409.60 ms /   151 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =    1469.38 ms /   101 tokens (   14.55 ms per token,    68.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2478.27 ms /    61 runs   (   40.63 ms per token,    24.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    3965.59 ms /   162 tokens\n",
      "Llama.generate: 16 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2872.92 ms\n",
      "llama_perf_context_print: prompt eval time =     686.90 ms /    18 tokens (   38.16 ms per token,    26.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     688.35 ms /    19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output1/page_3_img_1.png\n",
      "Saved: output1/page_3_img_2.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"pdf2.pdf\"\n",
    "    metadata = extract_pdf_content(pdf_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
